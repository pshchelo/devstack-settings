heat_template_version: queens

description: |
  Template which tests Octavia load balancing requests
  to members of Heat AutoScalingGroup.
  Instances must be running some webserver on a given app_port.
  Both HTTP and SSH access are load-balanced.
  Auto-scaling is driven by Aodh alarms besed on CPU utilization,
  derived from Gnocchi metrics that were collected by Ceilometer.

parameters:
  flavor:
    type: string
    description: Nova flavor to launch instances with
    constraints:
    - custom_constraint: nova.flavor
  image:
    type: string
    description: Glance image to launch instances from
    constraints:
    - custom_constraint: glance.image
  username:
    type: string
    description: default user name on the image used (for SSH access)
  key_name:
    type: string
    description: Nova keypair name to put on the instances (for SSH access)
    constraints:
    - custom_constraint: nova.keypair
  subnet_cidr:
    type: string
    description: IPv4 addresses range to create a subnet for servers with
    constraints:
    - custom_constraint: net_cidr
  public_net:
    type: string
    description: Network from where Floating IPs are assigned
    constraints:
      - custom_constraint: neutron.network
  app_port:
    type: number
    description: port on which the web app will run on instances
    constraints:
    - range: {min: 1025, max: 65535}
  timeout:
    type: number
    description: Time to allow instances to finish provisioning in seconds
    constraints:
      - range: {min: 1}
  min_asg_size:
    type: number
    description: minimal number of instances in auto-scaling group
    constraints:
      - range: {min: 1}
  max_asg_size:
    type: number
    description: maximal number of instances in auto-scaling group
    constraints:
      - range: {min: 1}
  launch_asg_size:
    type: number
    description: number of instances in auto-scaling group to create at launch time
    constraints:
      - range: {min: 1}
  cooldown:
    type: number
    description: autoscaling cooldown timeout in seconds
    constraints:
      - range: {min: 1}

resources:

  net:
    type: OS::Neutron::Net

  subnet:
    type: OS::Neutron::Subnet
    properties:
      network: { get_resource: net }
      cidr: { get_param: subnet_cidr}

  router:
    type: OS::Neutron::Router
    properties:
      external_gateway_info:
        network: { get_param: public_net }

  router_iface:
    type: OS::Neutron::RouterInterface
    properties:
      router: { get_resource: router }
      subnet: { get_resource: subnet }

  sec_group:
    type: OS::Neutron::SecurityGroup
    properties:
      rules:
      - remote_ip_prefix: 0.0.0.0/0
        protocol: tcp
        port_range_min: { get_param: app_port }
        port_range_max: { get_param: app_port }
      - remote_ip_prefix: 0.0.0.0/0
        protocol: tcp
        port_range_min: 22
        port_range_max: 22

  asg:
    type: OS::Heat::AutoScalingGroup
    depends_on: router_iface
    properties:
      desired_capacity: { get_param: launch_asg_size }
      max_size: { get_param: max_asg_size }
      min_size: { get_param: min_asg_size }
      cooldown: { get_param: cooldown }
      resource:
        type: OS::Test::NeutronAppServer
        properties:
          mdata: {"metering.server_group": {get_param: "OS::stack_id"}}
          image: { get_param: image }
          flavor: { get_param: flavor }
          key_name: { get_param: key_name }
          net: { get_resource: net}
          subnet: { get_resource: subnet}
          sec_group: { get_resource: sec_group }
          app_port: { get_param: app_port }
          app_pool_id: { get_resource: app_pool }
          ssh_pool_id: { get_resource: ssh_pool }
          timeout: { get_param: timeout }

  scale_up:
    type: OS::Heat::ScalingPolicy
    properties:
      adjustment_type: change_in_capacity
      auto_scaling_group_id: { get_resource: asg }
      scaling_adjustment: 1
      cooldown: 60

  scale_down:
    type: OS::Heat::ScalingPolicy
    properties:
      adjustment_type: change_in_capacity
      auto_scaling_group_id: { get_resource: asg }
      scaling_adjustment: -1
      cooldown: 60

  cpu_alarm_high:
    type: OS::Aodh::GnocchiAggregationByResourcesAlarm
    properties:
      description: Scale up if CPU utilization is growing fast
      metric: cpu
      aggregation_method: rate:mean
      granularity: 60
      evaluation_periods: 1
      threshold: 3000000000   # MAGIC! number, https://review.opendev.org/c/openstack/ceilometer/+/799963/1/ceilometer/compute/virt/libvirt/inspector.py#247
      resource_type: instance
      comparison_operator: gt
      alarm_actions:
        - str_replace:
            template: trust+url
            params:
              url: {get_attr: [scale_up, signal_url]}
      query:
        list_join:
          - ''
          - - {'=': {server_group: {get_param: "OS::stack_id"}}}

  cpu_alarm_low:
    type: OS::Aodh::GnocchiAggregationByResourcesAlarm
    properties:
      description: Scale down if CPU utilization is dropping fast
      metric: cpu
      aggregation_method: rate:mean
      granularity: 60
      evaluation_periods: 1
      threshold: 1500000000  # MAGIC! number, https://review.opendev.org/c/openstack/ceilometer/+/799963/1/ceilometer/compute/virt/libvirt/inspector.py#247
      resource_type: instance
      comparison_operator: lt
      alarm_actions:
        - str_replace:
            template: trust+url
            params:
              url: {get_attr: [scale_down, signal_url]}
      query:
        list_join:
          - ''
          - - {'=': {server_group: {get_param: "OS::stack_id"}}}

  lb:
    type: OS::Octavia::LoadBalancer
    properties:
      vip_subnet: { get_resource: subnet }

  app_listener:
    type: OS::Octavia::Listener
    properties:
      loadbalancer: { get_resource: lb }
      protocol: HTTP
      protocol_port: 80

  app_pool:
    type: OS::Octavia::Pool
    properties:
      listener: { get_resource: app_listener }
      lb_algorithm: ROUND_ROBIN
      protocol: HTTP

  health_monitor_app:
    type: OS::Octavia::HealthMonitor
    properties:
      pool: { get_resource: app_pool }
      delay: 3
      type: HTTP
      timeout: 3
      max_retries: 3

  ssh_listener:
    type: OS::Octavia::Listener
    properties:
      loadbalancer: { get_resource: lb }
      protocol: TCP
      protocol_port: 22

  ssh_pool:
    type: OS::Octavia::Pool
    properties:
      listener: { get_resource: ssh_listener }
      lb_algorithm: ROUND_ROBIN
      protocol: TCP

  floating_ip:
     type: OS::Neutron::FloatingIP
     properties:
       floating_network: { get_param: public_net }
       port_id:
         { get_attr: [ lb, vip_port_id ] }

outputs:

  app_lb_url:
    description: URL of the loadbalanced app
    value:
      str_replace:
        template: http://IP_ADDRESS
        params:
          IP_ADDRESS: { get_attr: [ floating_ip, floating_ip_address ] }

  ssh_lb_url:
    description: command for the loadbalanced SSH access
    value:
      str_replace:
        template: ssh -i KEY.pem USER@IP_ADDRESS
        params:
          IP_ADDRESS: { get_attr: [ floating_ip, floating_ip_address ] }
          KEY: { get_param: key_name }
          USER: { get_param: username }

  scale_up_hook:
    description: POST to this URL for manual scale up
    value: {get_attr: [scale_up, signal_url]}

  scale_down_hook:
    description: POST to this URL for manual scale up
    value: {get_attr: [scale_down, signal_url]}

  gnocchi_query:
    value:
      str_replace:
        template: >
          openstack metric measures aggregation --resource-type instance
          --query 'server_group="stackval"'
          --granularity 60 --aggregation rate:mean -m cpu
        params:
          stackval: { get_param: "OS::stack_id" }
    description: >
      This is a Gnocchi query for statistics on the cpu_util measurements about
      OS::Nova::Server instances in this stack. The --resource-type select the
      type of Gnocchi resource. The --query parameter filters resources
      according to its attributes. When a VM's metadata includes an item of the
      form metering.server_group=X, the corresponding Gnocchi resource has a
      attribute named server_group that can queried with 'server_group="X"' In
      this case the nested stacks give their VMs metadata that is passed as a
      nested stack parameter, and this stack passes a metadata of the form
      metering.server_group=X, where X is this stack's ID.
